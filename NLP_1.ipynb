{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vaiUvvTytTgK",
        "RVAG17u9UejS",
        "FA-2QyATUW4K",
        "aTkIfKtEUh9Z",
        "nPfUE2NMaYYS"
      ],
      "authorship_tag": "ABX9TyMbI4dEe+PNWYJzKCj7lERz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman-singanamala/Kaggle/blob/main/NLP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "vaiUvvTytTgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmeIFl-nFOSK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paragraph"
      ],
      "metadata": {
        "id": "RVAG17u9UejS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
        "               the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
        "               We have not grabbed their land, their culture, \n",
        "               their history and tried to enforce our way of life on them. \n",
        "               Why? Because we respect the freedom of others.That is why my \n",
        "               first vision is that of freedom. I believe that India got its first vision of \n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "lwUqKZ6BFUYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "[Refer](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4)"
      ],
      "metadata": {
        "id": "FA-2QyATUW4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert paragraph into different sentences\n",
        "# Tokenizing sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "sent= pd.DataFrame(sentences)\n",
        "sent\n",
        "# Tokenizing words\n",
        "words = nltk.word_tokenize(paragraph)\n",
        "words"
      ],
      "metadata": {
        "id": "Gd8IgEDujJKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "**Problem with stemming** :  Produced intermediate representation of the word may not have any meaning.\n",
        "<br>\n",
        "[Refer](https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/)"
      ],
      "metadata": {
        "id": "aTkIfKtEUh9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords \n",
        "## we use stopwords  because words like {them, them, of, .. etc} are repeated more \n",
        "## stops remove these kind of words which do not have much application\n",
        "\n",
        "\n",
        "## Tokenize\n",
        "sentences= nltk.sent_tokenize(paragraph)\n",
        "stemmer= PorterStemmer()\n",
        "\n",
        "## stemming\n",
        "for i in range(len(sentences)): # for all the sentences\n",
        "    words=nltk.word_tokenize(sentences[i]) # word tokenize from each sentence.\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]= ' '.join(words)\n",
        "\n",
        "\n",
        "pd.DataFrame(sentences)"
      ],
      "metadata": {
        "id": "r7iG0hXwnr2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "<br>\n",
        "\n",
        "[Refer](https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/)\n"
      ],
      "metadata": {
        "id": "nPfUE2NMaYYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words= nltk.word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]= ' '.join(words)\n",
        "\n",
        "pd.DataFrame(sentences)"
      ],
      "metadata": {
        "id": "Hj7PQogiX0oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions"
      ],
      "metadata": {
        "id": "liUNTAeSgzIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# match a word at the beginning of a string --match\n",
        "result=re.match('Apple',r'Apple Ball cat')\n",
        "print(f\"Match function result: {result}\")\n",
        "\n",
        "\n",
        "# search for the patter in a given string\n",
        "sent1= re.search(\"cat\", r'Apple Ball cat')\n",
        "print(f\"search result : {sent1}\")\n",
        "\n",
        "\n",
        "# findall -- return all the occurences of the pattern from the string.\n",
        "sent2= re.findall(\"Apple\", r\"Apple ball Apple cat Apple\")\n",
        "print(f\"Findall result : {sent2}\")\n",
        "\n",
        "\n",
        "# \\b \n",
        "# return a martch here the specified pattern is at the beginning or at the end of a word\n",
        "str= r'Apple ball Apple cat Apple'\n",
        "a=re.findall(r\"pple\\b\", str)\n",
        "print(a)\n",
        "\n",
        "\n",
        "# \\d\n",
        "# \\d returns a match where the string contain digits (numbers from 0-9)\n",
        "# checks if the string contains any digits (numbers from 0-9)\n",
        "str = \"1 2 milliom billion abc def\"\n",
        "x=re.findall(\"\\d\", str)\n",
        "print(x)\n",
        "\n",
        "# \\D\n",
        "# check if the word character does not contain any digits (numbers from 0-9)\n",
        "y=re.findall(\"\\D\",str)\n",
        "print(y)\n",
        "\n",
        "\n",
        "#\\D+\n",
        "z=re.findall(\"\\D+\",str)\n",
        "print(z)\n",
        "\n",
        "\n",
        "# \\w+\n",
        "# helps in extraction of alphanumeric characters only (characters from a to Z, \n",
        "# digits from 0-9 and the underscore_character)\n",
        "w=re.findall(\"\\w+\",str)\n",
        "print(w)\n",
        "\n",
        "\n",
        "# \\W\n",
        "# return match at every non-alphanumeric character. Basically opposite of \\w\n",
        "u=re.findall(\"\\W\",str)\n",
        "print(u)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTLKlUwchSO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words"
      ],
      "metadata": {
        "id": "JnWD83uLd4VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps= PorterStemmer()\n",
        "wordnet= WordNetLemmatizer()\n",
        "sentences= nltk.sent_tokenize(paragraph)\n",
        "\n",
        "\n",
        "corpus=[]\n",
        "\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    review=re.sub('[^a-zA-Z]',\" \", sentences[i])  # remove all the symbols like \",\", \"?\", ..etc\n",
        "    review=review.lower() ## lower the letters\n",
        "    review=review.split()\n",
        "    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=\" \".join(review)\n",
        "    corpus.append(review)\n",
        "pd.DataFrame(corpus)\n",
        "\n",
        "# Creating he Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv= CountVectorizer(max_features=1000)\n",
        "X=cv.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X)"
      ],
      "metadata": {
        "id": "sa4HYf6nV1d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "M6G0XbUY3EsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "ps= PorterStemmer()\n",
        "wordnet= WordNetLemmatizer()\n",
        "sentences= nltk.sent_tokenize(paragraph)\n",
        "\n",
        "corpus=[]\n",
        "for i in range(len(sentences)):\n",
        "    review= re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    corpus.append(review)\n",
        "\n",
        "\n",
        "# Creating the TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "uZDDkXPABYjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvLSfo5t4VB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}